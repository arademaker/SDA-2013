
\section{The proposal}\label{sec:proposal}

As discussed in Section~\ref{sec:problems}, relational databases are
often hard to maintain and share. Also, the idea of having in-house
developed and closed source information systems is being increasingly
replaced by the concept of open source systems. In such systems the
responsibility of updating and creating new features is not sustained
by a single institution but usually by a whole community that share
knowledge and interests with associates. In this way the system is
keept up-to-date, accessible and growing much faster due to the
increased number of contributors. Such systems are usually compatible
with standards in a way to garantee their widely adoption.

In this context we suggest that CPDOC improves the way their rich
historical data is maintained, stored and shared. Our proposal
privileges open source systems and a lightweight, shared way of
dealing with data.

% It is important to mention that CPDOC has currently three different
% information systems. Althought in the first stage the pilot project
% will involve only the DHBB system, this article is about the whole
% vision of the CPDOC data migration. This system was chosen due to
% different administrative reasons, but also because it has the
% simplest data model.

Concretely, we propose the substitution of the three CPDOC systems by
the following technologies.

The Acessus and PHO systems data and files would be stored in an open
source institutional repository software such as
Dspace~\footnote{\url{http://www.dspace.org/}}, Fedora Commons
Framework~\footnote{\url{http://www.fedora-commons.org}} or a more
specialized repository management system like
ICA-AtoM~\cite{van2009ica}~\footnote{\url{https://www.ica-atom.org}}. In
this article we assume the adoption of Dspace with no prejudice of
theoretical modelling. The Acessus data model comprises personal
archives that contains one or more series (which can contain also
other series in a stratified hierarchy) of digitalized documents or
photographies. The PHO system data model is basically a set of
interviews grouped according to some defined criteria within the
context given by funded projects. For instance, a political event
could originate a project which involve interviewing many important
people taking part on the event.

% These projects are the motivation and origin of financial support
% for a series of interviews conducted by CPDOC team with a specific
% purpose.

It is possible to consider that Acessus and PHO systems are basically
responsible for maintaining collections of documents organized in a
hierarchical structure. In this way, one can assume that any digital
repository management system have all required
functionalities. Besides that, digital repository management systems
share features desired but not present in Acessus or PHO, such as: (1)
data model based on standard vocabularies like Dublin Core~\cite{dc}
and SKOS~\cite{skos}; (2) long-term data preservation functionalities
(tracking and notifications of chances in files); (3) fine-grained
access control policies; (4) flexible user interface for basic and
advanced queries; (5) compliance with standard protocols for
repositories synchronization and interoperability (e.g.,
OAI-PMH~\cite{oai}); (6) import and export functionalities throuth
standard file formats and protocols; and more.

When it comes to the DHBB system, its relational model is a couple of
tables that store metadata about the dictionary entries (stored in a
single text field of a table). The actual dictionary entries are
created and edited in text editors outside the system and imported to
the system after the whole process of creation and revision. In this
way, the system does not maintain the history of changes. The content
of the entries are saved in HTML~\cite{html} format. The HTML is
generated by the different text editors used making it not uniform,
clean or portable and long-term compatible.

The nature of its data suggests that DHBB entries could be easily
maintained as text files using a lightweight human-readble markup
syntax. The files would be organized in a intuitive directory
structure and kept under version control for coordinated and
distributed maintenance. The use of text
files~\footnote{\url{http://en.wikipedia.org/wiki/Text_file}} have a
couple of reasons. They are: easy to maintain using any text editor
(tool independent) allowing the user to choose the best editor for
him; conform to long-term standards by being software and plataform
independent; easy to be kept under version control by any modern
version control
system~\footnote{\url{https://en.wikipedia.org/wiki/Revision_control}.}
since they are comparable (line by line) and efficient at storing
information~\footnote{A text file of a DHBB entry has usually 20\% the
  size of a file DOCX (Microsoft Word) of the same entry.}.

It is interesting to notice that the adoption of a version control
system will allow all file exchanges be recorded and the whole process
be controlled without any need of sophisticated workflow systems,
following the methodology developed by open sources communities for
open source softwares maintenance. For instance,
Git~\footnote{\url{http://git-scm.com}.} is specially developed to
ensure data consistency and keeping track of chances, authorship and
provenance. The use of a version control system will improve the
current workflow of DHBB reviewers and coordinators, since presently
there is no aid system for this task, basically performed using
Microsoft Word text files and emails.

Most of the suggested technologies are of daily use to most people
with technical profile, such as software developers, but not very
familiar to people with non-technical profile. In this context, a big
challenge of this approach is to motivate the internal users of CPDOC
systems, i.e., CPDOC archives maintainers, to invest their time to
learn new technologies, instead of keeping their inefficient but known
way of dealing with data.

This proposal ideas are already implemented partially for evaluation
purpose only. Figure~\ref{fig:dia-1} illustrates the necessary steps
to full implement our proposal. In the next pagraphs we will briefly
describe each step.

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.7\textwidth]{migration.png}
  \caption{Migrating from relational databases to the proposed model}\label{fig:dia-1}
\end{figure}

In step (1) the relational database was exported to
RDF~\cite{rdf-primer} using the open source D2RQ~\cite{d2rq} tool. The
D2RQ mapping language~\cite{d2rq-map} allows the definition of a
detailed mapping from the current relational model to a graph model
based on RDF. We have already defined the mapping from the relational
model to RDF model which uses the standard translation from relational
to RDF model sketched out in \cite{dbANDrdf}. The mapping created so
far defers any model improvement to step (2) described below. Until
CPDOC team decide about the adoption of the proposed technologies and
for the abandonment of the current architecture, the generated RDF can
be updated whenever necessary using D2RQ.

In step (2) we will create a refinement of the graph data model
produced in step (1). The ideia of the refinement is to produce a data
model based on standard vocabularies like Dublin Core~\cite{dc},
SKOS~\cite{skos}, PROV~\cite{prov} and FOAF~\cite{foaf}. The use of
standard vocabularies will make it interchangeable with other models
and facilitate its adoption by service provides and users. In
Section~\ref{sec:model} we describe in more detail the idea of this
refinement. Moreover, this step will also help to better understand
all database model and its semantics.

In step (3) we produced one text files for each DHBB entry. Each text
file hold the text of the entry and its metadata.~\footnote{It is out
  of the scope of this article present the final format of these
  files. A more detailed technical report with all the necessary
  technical details is planned.} The files use YAML~\cite{yaml} and
Markdown~\cite{markdown} markup languages for describe the metadata
and the actuall content of the entry. We choose to adopt YAML and
Markdown because both languages are human-readable markups for text
files and are supported by almost all static site
generators~\footnote{We used Jekyll, \url{http://jekyllrb.com}, but
  any other static site generator could be used.}. The use of a static
site generator gives to the DHBB maintainers complete control over the
deployment of a DHBB browsable version. Using some scripts that we can
easly develop they could deploy a DHBB version in three steps: (a)
syncronized his repository with the repository of other contributors;
(b) check the text files to make sure that the necessary metadata are
presented and have the expected values; and (c) compile the files
generating a complete website that could be made available for the
public using a web server and even distributed in mirrors
servers. 
% Actually, each DHBB contributor could generate his one browsable
% DHBB for navigate through the DHBB entries.
Finally, note that step (3) was actally implemented to use the initial
version of the RDF produced in step (1). In the future, the code could
be easly adapted to use the final RDF model produced by step (2).

In step (4) the digital files and their metadata will be imported into
the digital repository management system. This step will be much
easier to be done using the RDF produced in step (2) than having to
access the original database. For complete this step we only have to
decide about the repository management system that will be adopted,
install it in a server and create the necessary scripts. Considering
that all digital repository management systems have detailed control
access mecanisms, we believe that both high and low resolution files
can be imported to the same digital repository making only the low
resolution open for public access. This is necessary mainly for
preserve bandwidth and because in general, the low resolution files
also contain some watermark and embeded metadadata.

% The migration of the data from relational databases to
% RDF~\cite{rdf} and a simple prototype for browse the data was
% implemented using Jekyll System~\footnote{\url{http://jekyllrb.com}}
% and query support was provided by Apache Solr~\cite{solr} was built
% and used.

The proposed architecture is presented in Figure~\ref{fig:dia-2} and
will be described briefly in what follows. Remember that one of our
main goals is to make the CPDOC archive collections available as open
linked data. Usually, this is accomplished by providing the data in as
RDF/OWL files for download and a SPARQL Endpoint~\cite{sparql} for
queries. Since data and evolve constantly, CPDOC teams would have the
agree on a periodical releases of the data. Besides the RDF/OWL files
and the SPARQL Endpoint, we believe that it is also important to
provide a lightweight and flexible web interface for end users browse
and query the data. This can be easily done using a static website
generator and Apache Solr~\cite{solr} for advanced queries. Note that
the produced website, the RDF/OWL files and SPARQL Endpoints are
complementaries outputs and potencially serve to different purpose and
users. As a modern index solution, Solr can provide much powerful and
fast queries support when compared to traditional relational database
systems. This sum up the ideas of the architecture. 

\begin{figure}[htbp]
  \centering
  \includegraphics[width=.7\textwidth]{new-architecture.png}
  \caption{The final architecture}\label{fig:dia-2}
\end{figure}

Finally, it is vital to stress the main contrast between the new
architecture and the current one. In the current CPDOC architecture
the data is stored in relational databases and maintained by
information systems. This means that any data modification or
insertion is instantaneous available for CPDOC website users. In spite
of that, this architecture has a lot of drawbacks as mentioned in
Section~\ref{sec:problems}. More important than that, the nature of
CPDOC data does not require continous updates, which means that the
cost of this synchronous \emph{modus operandi} is not
justified. Usually, CPDOC teams work on projects basis and new
collections, documents and metadata revisions are released
sporadically.

% The Acessus and PHO collections of documents, images, audio and video
% files will be stored in the DRMS. Users can consult the DRMS directly
% or the collections and items metadata could be exported and integrated
% with additional information (DHBB and complementary data) into the
% final model (one or more RDF files). DHBB editors could generate from
% the YAML+Markdown some RDF files.

% In step (1) a script is used to syncronize the triple store with the
% digital repository metadatas. In step (2) the DHBB files are processed
% to generate the static website... the PHO and Acessus digital
% repository is queried and a RDF file is produced with a dump of its
% current state. The RDF files generated in steps (1) and (2) are
% combined into a single RDF file that is imported to a triple store. In
% (3) the combined RDF file with a snapshot of CPDOC data is made
% available for download or queries through a SPARQL Endpoint provided
% by the Triple Store. In (4) a Solr Instance Index is updated, serving
% as a back-end to a lightweight web interface that provides faced
% search and browsing interfaces of an integrated view of CPDOC
% collections. As a modern web development framework, Solr can provide
% much better and fast queries support when compared to traditional
% relational database systems.

It is possible to notice that the steps of Figure~\ref{fig:dia-2} need
not be synchronized, even though CPDOC teams could agree to follow a
common data release schedule. For instance, if a specific release of
CPDOC complete archives is planned but the DHBB team was not able
deliver a new DHBB version in time, new RDF/OWL files release can be
delivered normally. It would contain the last versions delivered by
each team, with no prejudice to the others.

The result obtained so far encouraged us to proposed a complete data
model aligned with open linked data vocabularies. In the next section
we will present an outline of how we suggest to do such map.

% FICOU MUITO FORA DE CONTEXTO To make CPDOC data widely used and
% available, we also suggest that CPDOC should define a annual
% schedule for distribution snapshots of its archives in RDF
% format. The RDF file(s) could be offer for download and online
% available for queries in a triple store with a SPARQL endpoint. In
% the next section we further describe the proposal architecture.

% In step (2) scripts use the RDF file produced in the step (1) to
% migrate files and metadata of Acessus and PHO systems to the open
% source repository software DSpace. The mapping of Acessus data to
% Dspace is described in Table~\ref{tab:map}. The mapping of PHO
% interviews is basically consisted of linking each project to a
% collection and each interview to an item with the respectively
% bitstreams (digital audio or video files).

% \begin{table}[htbp]
% \centering
% \begin{tabular}{rcl}
% Acessus &  & Dspace \\ \hline
% personal archives & $\to$ & communities \\
% series & $\to$ & collections \\
% documents or photographies metadata & $\to$ & items \\ 
% documents or photographies files & $\to$ & bitstreams \\ \hline
% \end{tabular}
% \caption{Mapping from Acessus to Dspace}\label{tab:map}
% \end{table}

% In step (3) the RDF model created is improved based on linked data
% concepts, i.e., considering the adoption of standard vocabularies and
% ontologies such as FOAF~\cite{foaf}, SKOS~\cite{skos}, Dublin
% Core~\cite{dc} and PROV~\cite{prov}. This refined RDF file would be
% available in form of periodic releases of CPDOC's collections that
% would be much more interoperable and useful for service providers.

% In step (4) Markdown~\cite{markdown} files are created using
% YAML~\cite{yaml} metadata header for each DHBB entry. YAML is a
% lightweight markup language for metadata description in a
% human-readable text file, while Markdown allows people to write text
% using an easy-to-read, easy-to-write plain text format that can be
% converted to structurally valid XHTML~\cite{xhtml} (for online use) or
% PDF (for printing). These files can be host in a distributed
% versioning control environment for collaborative maintenance. This can
% be achieved using Git~\cite{git}, a distributed version control system
% which is suitable for many different models of collaborations.

% Some tools are available for this kind of conversion and one of the
% most established is the D2RQ~\cite{d2rq}. Once the RDF is generated,
% several steps for improving and enriching the RDF file, checking for
% data consistency and gathering new data connections using the
% available knowledge sources (ontologies, dictionaries etc) is
% planned.  This environment of a group of files organized in a
% directory structure under control of a version control system and
% the RDF interface is meant to compose the new data storage for DHBB.

% Figure~\ref{fig:dhbb-ex} shows a fragment of a YAML+Mardown file from
% a DHBB entry, just as an example. Lines 1--16 are the YAML header with
% the metadata about the entry. The entry content is written in Markdown
% as observed from the line 17 on. In Line 18 it is possible to see that
% references to metadata fields can be made inside the body of the entry
% written in Markdown.

% \begin{figure}[thbp]
%   \centering
% \begin{lstlisting}[frame=single,numbers=left,basicstyle=\footnotesize\ttfamily]
% ---
% type: biliography
% created-by: 2010-03-04T17:55:58,83Z
% title: Assad Junir, Mario
% reviewer: Fulano
% author: Beltrano
% positions: 
%  - dep. fed. MG 1998-1999
%  - dep. fed. MG 2000-2002
%  - dep. fed. MG 2003-2007
% sources: 
%  - Camara dos Deputados; DIAP (Ago./06); Diario de Sao Paulo
%    (online) 29/10/2003. at http://oglobo.globo.com/diariosp.
%  - Portal Caparao (online) 01/jun/2007 e 15/maio/2008. Disp.
%    em http://www.portalcaparao.com.br.
% ---

% {{ title }}

% *Mario Assad Junior* nasceu em Manhuacu (MG) no dia 11 de 
% agosto de 1965, filho de [Mario Assad](/dhbb/mario-assad.html) 
% e de Nedi Vieira Assad. Seu pai foi deputado estadual em Minas 
% Gerais de 1967 a 1975 e de 1978 a 1983, secretario do Trabalho, 
% Acao Social  e Desporto de 1975 a 1978, deputado federal de 
% 1983 a 1991, secretario de Justica de 1991 a 1994 e prefeito 
% de Manhuacu entre 2001 e 2004.
% ...
% \end{lstlisting}
% \caption{YAML+Markdown file of a DHBB entry}\label{fig:dhbb-ex}
% \end{figure}

% Figure~\ref{fig:dia-2} shows how the CPDOC data is supposed to be
% maintained in our model. In step (1) a script is used to generate the
% current DHBB repositoty state RDF file for a DHBB maintainer. In step
% (2) the PHO and Acessus digital repository is queried and a RDF file
% is produced with a dump of its current state. The RDF files generated
% in steps (1) and (2) are combined into a single RDF file that is
% imported to a triple store. In (3) the combined RDF file with a
% snapshot of CPDOC data is made available for download or queries
% through a SPARQL Endpoint provided by the Triple Store. In (4) a Solr
% Instance Index is updated, serving as a back-end to a lightweight web
% interface that provides faced search and browsing interfaces of an
% integrated view of CPDOC collections. As a modern web development
% framework, Solr can provide much better and fast queries support when
% compared to traditional relational database systems.

% \begin{figure}[thbp]
%   \centering
%   \includegraphics[width=.9\textwidth]{diagrama2.png}
%   \caption{Data maintainance}\label{fig:dia-2}
% \end{figure}

% It is possible to notice that steps (1) and (2) of
% Figure~\ref{fig:dia-2} need not be synchronized, even though CPDOC
% teams could agree to follow a common data release schedule. For
% instance, if a specific release of CPDOC complete archives is planned
% but the DHBB team was not able deliver their version in time, the
% release can be delivered normally. It would contain the last versions
% delivered by each team, with no prejudice to the others.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "article_revA"
%%% End: 
