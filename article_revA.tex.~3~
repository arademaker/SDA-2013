\documentclass{llncs}

\usepackage{hyperref}
\usepackage{graphicx}
\usepackage{listings}

\begin{document}

\title{A linked open data architecture for contemporary historical archives}  
\author{Alexandre Rademaker\inst{1} 
  \and Suemi Higuchi\inst{2}
  \and D\'ario Augusto Borges Oliveira\inst{2}}
\institute{IBM Research and FGV/EMAp \and FGV/CPDOC}
\maketitle

\begin{abstract}
  This paper presents an architecture for historical archives
  maintenance based on Open Linked Data technologies and open source
  distributed development model and tools. The proposed architecture
  is being implemented for the archives of the Center for Teaching and
  Research in the Social Sciences and Contemporary History of Brazil
  (CPDOC) from Getulio Vargas Foundation (FGV).
\end{abstract}

\input{introduction}
\input{cpdoc}
\input{problems}
\input{proposal}
\input{mapping}

It is interesting to notice that all these file exchanges are recorded
and the whole process is controlled without any need of sophisticated
workflow systems, following the methodology developed by open sources
communities for open source softwares maintenance. Git is specially
developed to ensure data consistency and keeping track of chances,
authorship and provenance.

This new process can improve the current workflow of DHBB reviewers
and coordinators, since presently there is no aid system for this
task, basically performed using Microsoft Word text files and emails.

% The new database system composed by Markdown files and periodic RDF
% releases can be easily used for accessing data in an efficient
% way. The figure below illustrates the process. CPDOC and their
% collaborators would be responsible for creating Markdown files that
% would be automatically analyzed by a script for generating the RDF
% file, which would be refined and improved. This refined RDF would be
% made available to the web as linked open data, which would allow the
% information to be accessed and improved by the community. On the other
% hand, the RDF would also be stored in a triple store that allows for
% queries using many different GUI configurations (for instance using
% Solr) to easy the access of data stored. This schema follows a modern,
% open and scalable way of sharing and improving the data stored in
% CPDOC.

\section{Conclusion}

In this paper we presented a new architecture for CPDOC archives
creation and maintenance. The architecture targeted is based on open
linked data concepts and open source methodologies and tools. We
believe that despite the fact that CPDOC users would need to be
trained to use the proposed tools such as text editors, version
control softwares and command line scripts; this architecture would
give more control and easiness for data maintenance. Moreover, the
architecture allows knowledge to be easily incorporated to collections
data without the dependency of database refactoring. This means that
CPDOC team will be much less dependent from FGV's Information
Management and Software Development Staff.

% NÃO SEI SE VALE A PENA FLOREAR A CONCLUSÃO. CORTE, SE ESTIVER
% DEMAIS!  Por fim, entendemos que as possibilidades abertas nesse
% cenário transcendem em muito os limites do mundo digital e
% repercutem diretamente na nossa própria história em construção, na
% maneira como queremos colaborar para a mediação do saber. O ato de
% conhecer não se reduz a uma apreensão inerte de dados, ou tão
% somente advém de apreciações meramente lógicas. As pessoas conjugam
% faculdades cognitivas e perceptivas quando interagem entre si e com
% os recursos que as cercam, participando da construção do
% conhecimento sobre a realidade. O ambiente digital e de rede traz
% infinitas oportunidades de compartilhamento dessa história em
% construção, com desdobramentos na cultura e no aprendizado. Mas para
% que esse ambiente seja efetivamente transformador, é fundamental o
% papel das políticas de difusão e disponibilização dos registros
% históricos, bem como a busca pela sua melhor forma de acesso,
% garantindo a diversidade da qual não podem prescindir.

Finally, we believe that the possibilities offered in this scenario
far transcend the limits of the digital world and directly affect our
own \textit{history in the making}, the way we collaborate for the
mediation of knowledge. The act of knowing is not reduced to an inert
seizure of data, nor it comes from assessments merely logical. People
combine cognitive and perceptual faculties as they interact with the
resources that surround them, participating in the construction of the
knowledge over the reality. The digital environment and network brings
endless opportunities for sharing this history in the making, with
developments in culture and learning. But for this environment to be
effectively transformer, is crucial the role of policy dissemination
and availability of historical records, as well as the search for the
right way to access it, ensuring the diversity which can not do
without.

% é interessante mas nao sei se cabe aqui...  Many proposals of
% research concerning the use of lexical resources for reasoning in
% Portuguese using the data available in CPDOC are being carried out
% so as to improve the structure and quality of the DHBB entries.  Due
% the lack of space, we did not present in this paper.

\bibliographystyle{plain}
\bibliography{sda}

\end{document}



\section{Logics and Ontologies for Portuguese}

One of my main scientific interests is the logics arising from
knowledge representation of texts.  But to discuss logic and reasoning
with logical knowledge representations, in a way that is scalable and
not brittle, we must deal with language seriously. We need to be able
to build up syntactical and semantical representations, so that we can
build logical representations and reason with them, even when these
representations are very superficial.

We would like to repurpose some of the open source machinery developed
for understanding English, to short cut the development of language
processing and reasoning in Portuguese. In particular it would useful
to be able to bootstrap knowledge acquisition in Portuguese using the
knowledge base SUMO to organize meanings/concepts.

The goal of this self-contained, personal, research project is to do
so in the field of biographical features of historical characters in
recent Brazilian history. We provide some background and context for
the project, before a schematic description.


\section{Background: Textual Inference Logic at FGV}

Our research project started in August 2010. Financial support has
been provided by FGV, which has paid for visits by Dr. de Paiva and
Dr. de Melo to FGV.  The first visit by Dr. de Paiva was in September
2010 and the second by Dr. de Paiva and Dr. de Melo was in November
2011. To make the second visit more productive, we organized a
workshop at FGV titled ``Logics and Ontologies for
Portuguese''.~\footnote{\url{http://emap.fgv.br/events/nlp-2011/}.}
FGV has also paid for trips of Dr. Alexandre Rademaker for his
participation in international conferences.

In more detail, Alexandre Rademaker and I started talking about the
research in reasoning from language when he was doing his internship
with Natarajan Shankar at SRI, Menlo Park in 2009. I was working for
Cuil, Inc and got him interested in the idea of Processing Natural
Language to obtain logical formulas (in Description Logic, the subject
of his doctoral work) and in obtaining a reasoning logical framework
out of free-form text, using the work I had been doing at Xerox PARC's
Natural Language Technology and Theory (NLTT) previously.

Most of the work in processing natural language has been done for
English, but adapting some of this work to Brazilian Portuguese should
not be too difficult, we reasoned in 2010. While the work I did at
PARC encompassed a whole pipeline from reading text to using the
representations built in applications such as Question Answering or
Summarization, since we were promised the use of PARC's codebase for
academic purposes, our initial intention was simply to supply the
necessary Portuguese language resources to complement the PARC system,
as far as the more logical tasks were concerned.
 
To this end I visited the FGV in late 2010 for some brainstorming
sessions on what would be the most useful applications of NLP in
Portuguese, as far as FGV objectives were concerned. I tried to map
local Portuguese resources and tried to uncover relevant possible
collaborators. I also gave three lectures on the subject of what I
wanted to do and how (in very loose terms) this could be achieved.
 
From these conversations and given that fact that I was hoping to get
a Xerox PARC license to use their system XLE-Bridge for our project,
we decided that, as first step, we needed a lexical resource such as
Princeton's WordNet for Portuguese.

We knew there were already in existence three other projects
(\cite{wn1, wn2, wn3}) trying to provide a Portuguese WordNet, so we
first assumed we would be able to join one of these projects and use
their resources. Unfortunately this was not possible. None of the
groups wanted to produce open source code, freely downloadable, which
seemed to us the only way to grow up and keep improving this kind of
resource.  Thus we engaged in a project of our own, named
OpenWordNet-PT~\cite{GWA:2012:VCP:AR}, (or Open WordNet in
Portuguese), as none of the previous proposals seemed to want to post
openly and allow downloading of the resources they develop. Our
project and prototype were funded by FGV itself.

We came across the Global WordNet Association (GWA) and felt inspired
to obtain a small core version of a Portuguese WordNet by manually
translating the core concepts of the original Princeton word net, a
method suggested by the GWA. Through recommendation of Adam Pease,
then at Rearden Commerce, we were introduced to Gerard de Melo's work
on the Universal WordNet \cite{}, a large-scale multilingual lexical
database that seemed very relevant to our project. Universal WordNet
(UWN) organizes over 800,000 words from over 200 languages in a
hierarchically structured semantic network, providing over 1.5 million
links from words to word meanings. Amongst these 200 languages
Portuguese is considered. This universal wordnet is bootstrapped from
the Princeton WordNet using graph-theoretic techniques, the core of de
Melo's doctoral work with G. Weikum at MPI. De Melo's multilingual
universal wordnet has a high level of precision and coverage, and it
can be useful in applied tasks such as cross-lingual text
classification. We realized that it could be very useful indeed in our
project to bootstrap the resources necessary to creating our version
of a Portuguese WordNet and decided to invite Gerard to talk about his
research at FGV in Rio de Janeiro in November 2011.

After a week of intense discussions and many hours of lecture
preparation and presentation in Rio, it was decided that Gerard would
prepare a big file from the original UWN, which I called a
``projection" of the universal worldnet, consisting of the synsets in
Portuguese and Spanish, which we would then manually curate, with the
help of Rafael Hauesler, Alexandre's intern at FGV. We also had
beginnings of discussions of how the data could and should be used,
but no conclusive decisions were taken.

Later on Alexandre Rademaker went to the Global WordNet Association
meeting in Japan and talked about the project, the paper is in the
{\it Proceedings of Global Wordnet Conference} \cite{}.  Recently
Francis Bond added the OpenWordNet-PT to his online interface to many
open wordnets ``Open Multi-lingual Wordnet (1.0)".

Thus our first version of OpenWordNet-PT launched in August 2012 with
7422 adjectives, 55951 nouns, 1726 adverbs and 7155 verbs. It can be
searched online and/or downloaded at the site. It has a collection of
problems, as any first version of a resource, for example we have some
duplicates, as a by-product of how the entries (sunsets) were
created. But all in all it is a remarkable result for such a short
project, so far.

\section{Context for Resources and Reasoning}

In the long term, as results from this project, we envisage a
collection of several modules, helping to produce logical forms from
sentences, including, perhaps, Brazilian Portuguese versions of
WordNet, SUMO, NOMLEX~\cite{nomlex}, VerbNet~\cite{verbnet}, specific
ontologies (of e.g. politicians roles) for the domain as well as a
lexicon of multiword expressions adapted to the domain we want to
concentrate on, historical biographies. In the meanwhile we believe we
need to organize:
%The contents of the modules are:

\begin{enumerate}

\item Online searchable Corpora. The archives of FGV plus collections
  of texts freely available, preferably with various genres.

  In particular the would like to process a downloaded version of the
  Portuguese Wikipedia to make side by side comparisons with the CPDOC
  dictionary of historical characters (DHBB). Both Wikipedia infoboxes
  and its internal links network can be used to induce similar
  collections in the CPDOC dictionary, if there is a great
  intersection of the resources, which we believe there will be. For
  general comparison and baseline we can use other corpora available
  from NILC~\footnote{\url{http://www.nilc.icmc.usp.br/}}, for
  instance.

\item WordNet-like lexical database. There are several proposals for
  WordNet-like ontologies for Portuguese, but as described above, none
  seem to work for us. The papers by Teixeira et al and Oliveira et al
  \cite{Oliveira2008,teixeira2010} describe the situation (as in 2010)
  and provide some links.
  
  As mentioned above we now have a first version of our own resource,
  OpenWN-pt, freely available at \url{} and downloadable, but whose
  quality and accuracy is not, as yet, measured. Producing credible
  measurements of the quality of OpenWN-PT is one of our first goals
  in the new project.

\item VerbNet-like lexical knowledge base. We believe a Brazilian
  attempt at a VerbNet-like resource is under development at NILC,
  USP-SC, but no promises of open access have been made.
 % effort for Portuguese, yet, despite the fact that the main architect
 % of VerbNet~\cite{verbnet}, Karin Schuler, is Brazilian. 
  
  Also we plan to partner with the group of Prof. Leonel Alencar from
  the Universidade Federal do Cear\'a to work on the more syntactical
  aspects of our project and they apparently are planning a different
  version of a Portuguese VerbNet. If this turns out to be the case,
  we hope to be able to use it in our research.
 
\item Named Entity Recognizer. Apparently, there are many entity
  recognizers available for Portuguese. There was a couple of
  competitions between these, HAREM is the name of the competition,
  lead by Diana Santos. The second edition of the competition has
  edited a book, and slides for the presentations can be found at the
  competition
  website.~\footnote{\url{http://www.linguateca.pt/aval_conjunta/HAREM/EncontroSegundoHAREM.html}}
  More about HAREM at \url{http://www.linguateca.pt/HAREM/}.
  
  But other recognizers such as Rembrandt \cite{}(\url{}) and
  Wikipedia Miner seem also interesting possibilities. We need to
  decide which recognizer we will be wanting to use and then test and
  measure its coverage and accuracy.
  % A possible collaborator of our project Dr Christian Aranha has
  % experience with NERs in Portuguese, but since Dr Aranha has a
  % small
  % company selling language tools and we want everything in our
  % project
  % to be freely available we have not yet uncovered how to make the
  % proposed collaboration work.
 
\item 
  % SUMO in Portuguese. We believe that our students and contributors
  % can, mostly, deal with our chosen ontology SUMO and its
  % development environment Sigma in English, but we plan to translate
  % parts of the domain ontology specifically dedicated to historical
  % figures to Portuguese. More importantly
  We hope to leverage SUMO as a formal {\em lingua franca}, to help
  validate our automatic translation of Princeton's English WordNet
  into Portuguese.

  SUMO has the beginnings of a domain ontology dedicated to historical
  figures, started as part of a project to cover all of Wikipedia
  infoboxes concepts in SUMO under Adam Pease's direction. Many of the
  Wikipedia infoboxes are about historical figures. Infoboxes contain
  only some basic information about historical figures, like name,
  date of birth, date of death, official title or position and
  predecessor and successors in that position. We reckon that
  recognizing this kind of information in all biographies in the FGV
  dictionary will help make it more informative, as it will provide
  means of navigation towards both other people in similar positions
  or to the places and times where events happened. Eventually we
  would like to bring in also the events themselves and relationships
  between them, but the first step ought to be simply connecting
  people with their less controversial data, such as data and place of
  birth and death.

  SUMO has already a specialized small ontology called Biography.kif
  created for the purpose of mapping Wikipedia's infobox biographical
  information to SUMO, but many of the positions/professions in
  Wikipedia info boxes are not yet mapped into SUMO concepts.  SUMO
  also has the basic translations of its main concepts (around a
  thousand) to Portuguese, but we believe this is not enough for the
  process of cleaning up and improving our biographical data in
  Portuguese. While we could, in principle, translate the whole SUMO
  ontology to Portugues, it is not clear to us (a this stage) that
  this would be a good use of resources.

  We hope that {\bf triangulation} between the data in the OpenWN-PT,
  the data in the FGV dictionary DHBB, the data in the Portuguese
  Wikipedia and the data in the English Wikipedia will help us improve
  the quality of the the resources.
  
  %if we can have the human support we are hoping for. 
  For the triangulation of the data itself we hope to count on the
  expertise of Dr Gerard de Melo, who visited FGV in Nov 2011 and
  seems enthusiastic about the project. Dr de Melo brings a formidable
  experience with the lexical knowledge base YAGO, where electronic
  dictionaries and thesauri in several languages were used with the
  backbone of WordNet synsets taxonomies as a means for the
  construction of multilingual WordNets and a generic ontology
  extracted from Wikipedia.

\item A small side project that we are also engaged in is a collection
  of small hand-curated lexicons such as NOMLEX-BR. NOMLEX in English
  is a well-established project out of New York City University that
  has become a de facto baseline for the study of nominalizations in
  many languages. The project is open source and freely distributable,
  so we intend to provide our own (preliminary) version of NOMLEX-BR
  as they do, as soon as possible.  Nominalizations are important for
  knowledge representation as ontologies tend to concentrate their
  content in noun hierarchies, paying much less attention to verbs,
  which usually correspond to processes. Nominalizations (sometimes
  called deverbals in the AI literature) are an important mechanism
  that languages have to transform verbs into corresponding
  nouns. Investigating how nominals (or deverbals) work both in
  English and in Portuguese will improve SUMO itself and it will help
  us with the modeling of events in language, in general.
  
\item Finally we want to explore comparing and combining our system
  with complementary frameworks and knowledge sources.
  Prof. Dr. Vladia Pinheiro, another possible collaborator in the
  project, has created an ontological representation based on a
  common-sense knowledge base called ConceptNet. We wondered if we
  could set up a joint project with Dr Pinheiro to have her systems
  SIA (Semantic Inferialist Analyzer) and SIM (Semantic Inferialist
  Model) compare Wikipedia violent events in Brazilian History to her
  corpus of occurrences.  Dr Pinheiro's system at the moment is tuned
  to deal with crime in the streets of Fortaleza, and has vocabulary
  to deal with these kinds of violent events.
\end{enumerate}

Among the resources we do plan to investigate as the project develops,
as we know they will be needed at some later stage are:

\begin{enumerate}
\item Temporal expressions recognizers
\item Geographic and spatial expressions recognizers
\end{enumerate}

This is a fairly ambitious proposal, but it can be cut down to size,
to yield immediate ``low hanging fruit''. In the following section we
describe in more detail where we plan to concentrate the immediate
efforts of this collaboration.

% To analyze FGV's historical texts, we would need an LFG (Lexical
% Functional Grammar) for Portuguese, coupled with some lexical
% resources. In particular we need some collections of texts reasonable
% clean (corpora), we need electronic dictionaries and thesauri
% (Jspell~\footnote{\url{http://www.jspell.com/}} and/or
% aspell~\footnote{\url{http://aspell.net/}} might be enough), we need a
% morphological analyzer and a named entity recognizer. We also need
% lexical resources like a Portuguese versions of WordNet, NOMLex and
% VerbNet.
% These resources are not particularly easy to obtain or to put to work
% together. Some of the resources are independent and could be pursued
% in parallel.

\section{Resources for Reasoning}

Our new joint collaboration ``Resources and Reasoning" has four main
immediate goals, namely:

\begin{enumerate}
\item Measure and improve the accuracy of our own OpenWordNet-PT;

\item Choose and implement a version of named entity recognition and
  of a tokenizer for Brazilian Portuguese;

\item Create an ontology of salient concepts associated with
  historical biographies in SUMO and show its suitability for
  modelling FGV's Dictionary of Brazilian Historical Biographies, the
  DHBB.

\item Demonstrate the feasibility of a prototype for reasoning about
  relationships between historical characters in the DHBB, using
  SUMO-Sigma and the resources we have created.
\end{enumerate}

Our main goal is the last one, where we hope to do some logical
reasoning using the ontology created in item 3.

\section{Brazilian Dictionary of Historical Biographies Mining}

The main application we envisage for our work in OpenWordNet-PT is
simple, but groundbreaking.  We want to read the entries in the DHBB
and extract from them the main SUMO concepts referenced.  Using these
main SUMO concepts we want to bootstrap a fledgeling Ontology of
Historical Biographies, already started and documented in the
Development section of SUMO.  From the analysis of the SUMO concepts
uncovered in the biographical entries, we want to discover new
relationships between the historical characters described in the DHBB.

Here are some examples of the kinds of questions that this association
of text of biography entry with collections of SUMO concepts will
allow us to answer:

\begin{itemize}
\item Amongst Brazilians first rank politicians how many are from Sao
  Paulo?
\item Has the proportion of ``Paulistas'' increased of decreased since
  the 1930's? SInce 1965 when Brasilia became the Capital of Brazil
  has the proportion changed?
\item What are the important Brazilian political families,
  corresponding to the Kennedy's, the Bush's, etc?
\item What are the prevalent occupations among Brazilian historical
  figures? Are they all lawyers by training?
\item Are there Universities or Colleges who clearly create
  ``political leaders''?
\item Are there Universities or Colleges who clearly create ``economic
  leaders'' or entrepreneurs?
\end{itemize}

To get to the stage of asking and answering these questions we need
first to actually provide the so-far potential mapping from words in
Portuguese to SUMO concepts.

There is such a mapping in English, one of the reasons SUMO is a good
choice for us. This mapping is described in ``Linking Lexicons and
Ontologies: Mapping WordNet to the Suggested Upper Merged Ontology'',
Ian Niles and Adam Pease
(http://home.earthlink.net/~adampease/professional/Niles-IKE.pdf). Since
Portuguese synsets are linked to English synsets we can possibly use
this composite mappings, but the disambiguation problem might hit us
hard. We need to check.

If the composite mappings work, via our OpenWordNet-PT and the ability
to relate synsets in Portuguese to synsets in English, we can perhaps
`inherit' a high quality, hand-coded, WordNet to SUMO mapping.

To be able to use this mapping we need first to be able to tokenize
our sentences and we need to be able to detect ``named entities" in
our corpora. We also need to recognize "compound expressions" that are
essential to our domain such as ``deputado federal" or ``primeira
dama". I believe that producing lexicons for this specific domain
should be feasible in a one-year long full time project, which may
take longer, if done in short spells.

Many more lexical resources would be helpful for reasoning about
free-form text. We mention here two other resources that we have
started working on, a Portuguese version of NOMLEX and a Portuguese
version of existential change verbs. Even if we are using superficial
(not deep) processing of the language, that is, even if we do not
parse sentences and try to find all the information conveyed, it is
rather essential to distinguish verbs that signify non-existence of
concepts. An example would be ``The meeting was cancelled''. In the
context of this utterance there is no real meeting, as that was
cancelled, but the usual meddling of the sentence would create a
concept corresponding to this nonexistent meeting. This non-existence
needs to be accounted for even in shallow processing of language.

\section{Summary}

This unusual proposal for a part-time Senior Visiting Professorship is
based in part on a research program me already in course with
Prof. Alexandre Rademaker.

This research programme aims at developing tools and resources for
Brazilian Portuguese that would allow us to construct and reason with
logical forms extracted from unstructured text. The research program
me was originally called ``Textual Inference Logic'' (TIL) and it is
now called ``Resources and Reasoning in Portuguese''.

The goals are the same, the methods have hardly changed, but we are
getting clearer on what we can accomplish given a certain amount of
time. We aim to use SUMO and its theorem prover Sigma as ontology
building and improving tools for our knowledge bases based on CPDOC's
Historical Dictionary of Brazilian Biographies (DHBB). The project has
already produced a preliminary version of a Brazilian Portuguese
lexical base OpenWN-PT in the style of Princeton's WordNet and we plan
to improve this lexical base and use it to reason about the
biographies of historical politicians in the DHBB.


% Also some corpora available from
%   Linguateca~\footnote{\url{http://www.linguateca.pt/}}. In
%   particular, Brazilian colleagues have worked hard to produce TEP, an
%   electronic thesaurus for Brazilian Portuguese, \cite{mazieroetal}.

% \item Wordnet lexical-semantic database. There are several proposals for
%   WordNet-like ontologies for Portuguese. The papers by Teixeira et al
%   and Oliveira et al \cite{Oliveira2008,teixeira2010} describe the
%   situation and provide links.

% \item VerbNet-like lexical ontology. We have not seen a similar effort
%   for Portuguese, yet. But the main architect of VerbNet (Karin
%   Schuler) is Brazilian, she might be interested in helping out.

% \item Morphological analyzer. There are apparently many available.  So
%   much so that there is/was a couple of competitions to discover the
%   best one, organized by Diana Santos. A preliminary write-up is
%   available from \cite{santos2003}. Perhaps PALAVRAS is a good
%   option?~\footnote{\url{http://beta.visl.sdu.dk/visl/pt/parsing/automatic/}}
 
% \item Named Entity Recognizer. There are apparently many available. So
%   much so that there is/was a couple of competitions to discover the
%   best one, HAREM is the name of the competition, lead by Diana
%   Santos. The second edition of the competition has edited a book, and
%   slides for the presentations can be found at
%   \url{http://www.linguateca.pt/aval_conjunta/HAREM/EncontroSegundoHAREM.html}.
%   More about HAREM at \url{http://www.linguateca.pt/HAREM/}.

% \item A LFG grammar. If we have an LFG grammmar we might be able to
%   use the XLE(Xerox Language Engine) directly, hence we do not need a
%   (deep) parser of our own. Similarly we might be able to use the
%   transfer semantics and the AKR (abstract knowledge representation)
%   creation facilities.

% \item Anaphora Resolution. This is required for dealing with more than
%   one sentence, as we need to recognize the entities referred to in
%   previous sentences.
% \end{enumerate}

% Note that if we do not have an LFG grammar, then we need a parser, a
% semantics module and some sort of knowledge representation module,
% which brings the numbers from six to nine modules.  The corresponding
% system for English, together with some lessons learned, has been
% described in \cite{depaiva:LFSA}.

% Among the resources we do not investigate, despite knowing that they
% will be needed at some later stage are:

% \begin{enumerate}
% \item Multiword recognizer 
% \item Temporal expressions recognizer
% \end{enumerate}

% A FrameNet-like resource for Portuguese may also be considered. We hope
% to start with a version of WordNet and some version of VerbNet, that
% we need to construct.






\section{Outline} 
\begin{itemize}
\item Intro
\item Dictionary
\item Using OpenWN-PT, SUMO
\item Categories for Co-Intuitionistic Logic
\end{itemize}

\section{ideas from emails past}

write to Bento, PAlmira Marrafa, Antonio Branco e Vladia and Leonel about helping to improve OpenWN-PT.
Need a email dedicated to reporting issues
Need an email for MArina, Sandra and Henrique.

need to write to Francis asking for 
1. glosses
2. adding an email for reporting mistakes.
Done, reply is good
-----------------
Quality of OpenWordNet-PT
The best strategy is therefore to summarize the description of resources in the end of
such projects and check validity of information in all documents that will be part of
the documentation.
The terminology used in the resource description should be also explicitly defined.
Even the meaning of terms that seem to be basic in the context should be tackled.
For example, synonymic set { synset { is the fundamental building block of wordnets.

The description of the data format in which the resource is provided plays also a
crucial role. As XML has become de facto standard for data interchange, it is natural
to make data available in XML and release the relevant DTD description. Data types
of XML entities and other constraints on the tag content should be also specified.
one of the most successful procedures to control the quality of
linguistic output is to implement a set of validation checks and regularly publish their
results. It holds especially for projects with many participants that are not under the
same supervision. Validation check reports together with the quantitative assessment
can serve as development synchronization points too.

All partners agreed to prepare and update resource description sheet"
for the wordnet they develop. Such a specification should contain at
least: ? description of the content of synset records and constraints
on data types; ? types of relations included together with examples; ?
degree of checking relations borrowed from PWN (see the note about the
expand model below); ? numbering scheme of different senses (random,
according to their frequency in a balanced corpus, from a particular
dictionary, etc.)  ? source of definitions and usage examples; ? order
of literals in synsets (corpus frequency, familiarity, register or
style char- acteristics).

1. All synsets contained in EuroWordNet base concepts have been included to
maximize the overlap between the two projects.
2. The set has been extended based on the proposals of all partners who added
synsets corresponding to the most frequent words in corpora and in various
dictionary definitions for their particular languages.
3. As an additional criterion, several noun synsets that had many semantic rela-
tions in the Princeton WordNet database have been added.

A special mechanism has been adopted to signalize lexical gaps  concepts that
are not lexicalized in a language. Such entries are labeled <NL/> in the BalkaNet
database and they should be ignored when working with a particular wordnet as a
monolingual resource.

Semi-automatic checks that need additional language resources to be integrated
are usually performed by each partner depending on the availability of the resources:
? spell-checking of literals, definitions, usage examples and notes;
? coverage of the most frequent words from monolingual corpora;
? coverage of translations (bilingual dictionaries, parallel corpora);
? incompatibility with relations extracted from corpora, dictionaries, or encyclo-
pedias.
----------------------

 Na linha de tentar construir uma WordNet-BR a partir da EN usando o Google Translation, ja tenho a WordNet 3.0 em RDF em uma triplestore. Esta semana acho que consigo terminar um script para usar o Google translation nos glosses.
 Google APi for glosses still good!
 scraping other wordnets in portuguese?
 email Bento now with stuff available on web and proposal of merge
 
 mas outra possibilidade 'e fazer uma ontologia SUMO de figuras historicas. acho que fazer a ontologia nao 'e dificil, a questao 'e como usar o resultado...
 
 mais uma, usar  verbnet to extract complement verbs and getthe brazilian ones that are the same.
 
 a gente tem a divisao do OpenWB-PT em nouns. verbs, adjectives and adverbs?
 qual e'?
 a gente tem os glosses separados? qdo a gente tem gloss ele aparece no nultilingual?
 
 sentiment lexicon in PT, sentilex
 http://label.ist.utl.pt/pt/downloads-pt.php
 

\section{History.kif, Biography.kif Brazilian Geography.kif?}

Need lists of names, christian and surnames, need list of places and institutions in Brazil. what does wikipedia know?

 I was told by Colin Baker of the framenet beginnings in Juiz de Fora
and indeed it seems a good idea to invite them.

From Information to Understanding: Moving Beyond Search In The Age Of Siri http://tcrn.ch/P3xtAJ
http://techcrunch.com/2012/07/28/from-information-to-understanding-moving-beyond-search-in-the-age-of-siri/
According to Google, the average user visits 22 sites prior to booking travel. Planning a trip can be unnecessarily frustrating and time consuming. What if planning your travel was as easy as interacting with a friend from the area you plan to visit, one whose on-the-ground recommendations cut through the clutter with aplomb, and who knows what you enjoy?

wikipedia on NER: Despite the high F1 numbers reported on the MUC-7 dataset, the problem of Named Entity Recognition is far from being solved. The main efforts are directed to reducing the annotation labor [7] ,[8] robust performance across domains[9][10] and scaling up to fine-grained entity types.[11][12]
At least two hierarchies of named entity types have been proposed in the literature. BBN categories, proposed in 2002, is used for Question Answering and consists of 29 types and 64 subtypes.[5] Sekine's extended hierarchy, proposed in 2002, is made of 200 subtypes.[6]
For traditional NER, the most popular publicly available systems are: OpenNLP NameFinder, Illinois NER system, Stanford NER system, and Lingpipe NER system. The Illinois NER reports 90.6 F1 on the CoNLL03 NER shared task data and the Stanford NER reports 86.86 F1.[16][17]

There are also several publicly available Wikification systems for identifying important expressions in the text and cross-linking them to Wikipedia. Most notably, Illinois Wikification system WM Wikifier and TAGME.

http://xldb.di.fc.ul.pt/Rembrandt/? for Portuguese open source



Cohan Sujay Carlos: The NIST information extraction tasks, especially
MUC and ACE, might be related to what you're working on. You can take
a look at the measures used in those tasks. Here's a paper on some ACE
evaluation metrics:
http://www.itl.nist.gov/iad/mig/publications/storage-paper/ACEXDOC-FinalPaperV3_NIST.pdf -
4 months ago Like


Vineet Yadav Apart from ACE evaluation matrices, you can also look at
MUC evaluation matrices(http://acl.ldc.upenn.edu/M/M92/M92-1002.pdf,
http://acl.ldc.upenn.edu/M/M93/M93-1007.pdf). You can look at Diana
Maynard(http://www.dcs.shef.ac.uk/~diana/) paper on evaluation of
information extraction system(Metrics for Evaluation of Ontology-based
Information Extraction). Diana Maynard works in University of
Sheffield on GATE information extraction
system(http://gate.ac.uk/). You can look at GATE evaluation matrics
(http://gate.ac.uk/sale/tao/splitch10.html#x14-25900010.1) and GATE
balanced metric score pluggin(
http://gate.ac.uk/sale/tao/splitch10.html#x14-28300010.6). You can
find more information about GATE balanced metrics score plugin in
Diana paper "Metrics for evaluation of ontology-based information
extraction" and "Benchmarking ontology-based annotation tools for the
semantic web." ). If you are creating training dataset for information
extraction system,then apart from precision, recall calculation, you
can also calculate inter-annotator agreement between the
annotators. you can use Cohen Kappa
agreement(http://en.wikipedia.org/wiki/Cohen\%27s_kappa) or
krippendrop alpha
(http://en.wikipedia.org/wiki/Krippendorff\%27s_Alpha) to calculate
inter-annotator agreement between different annotators.




The research proposed here is an extension of the linear functional programming 
paradigm. Most of the work concerning linear functional programming has used as 
its basic language the system of Intuitionistic Linear Logic, the fragment 
considered by Lafont. For this fragment, since derivations have a single 
conclusion, it is easier to consider a Natural Deduction formulation, and hence 
the analogy with standard functional programming is clearer. we intend to develop 
the computational interpretation of FILL. The challenge comes from the fact that 
since derivations in the system have multiple conclusions (to be thought of as 
terms to be evaluated in parallel) the concept of Natural Deduction becomes much 
more involved. There are a few attempts in the literature at notions of multiple 
conclusions Natural Deduction, but none of the solutions proposed to date seems 
satisfactory. The motivation for seeking a multiple conclusion system seems 
clear: this corresponds to an intrinsically parallel version of (linear) 
functional programming and as such it will help with resource control in a 
parallel-based environment. In other words FILL has potentially the ability 
to initiate a new style of functional programming where parallelism is directly 
built into the derivations. We should try to develop this idea into a working 
implementable system either along the lines of Mackie's linear programming 
language Lilac, \cite{Mackie91}, or along the lines of an extended version of 
D. Miller's logic programming language Forum, \cite{Miller94}, which is based 
on linear logic.

Some work has already been done on FILL. The original presentation of the system, 
in \cite{HylanddePaiva93}, had a mistake and the system as presented did not enjoy 
the cut-elimination property. In a paper by Bra\"{u}ner and de Paiva, 
\cite{BraunerdePaiva96}, as well as correcting the mistake in the original paper 
and extending the system to deal with exponentials, we introduced a different 
formal system describing the intuitionistic character of FILL and provided a full 
proof of the cut elimination theorem. The idea of a notion of dependency between 
formulae within a given proof is the basis of our new formal system and it seems 
of independent interest. For example, the formalisation of this notion seems a 
challenge to formalisms like Milner's new action calculi. The procedure for cut 
elimination applies to Classical Linear Logic, and we can (with care) restrict our 
attention to the subsystem FILL. Historically, our work on FILL grew out of a 
search for a fragment of Classical Linear Logic with a polynomial time cut 
elimination algorithm, analogous to the aim of Girard's Light Linear Logic, 
\cite{Girard94}. 

Having solved the problem of how to formulate the pure logic for FILL in a way 
that guarantees cut elimination, we are keen to tackle the other problem raised 
in the original paper. To quote the last paragraph of \cite{HylanddePaiva93}:
\begin{quote}
{\em It still remains to give a satisfactory account of the computational meaning 
of the term calculus for FILL. With other colleagues we have been trying to give 
an interpretation in terms of processes, but we rather hope that a number of 
different interpretations will emerge. We should like to end by recording our 
belief in the significance of FILL. It seems to provide a context for considering 
different interpretations of functional programming, and as such to deserve 
further attention.}
\end{quote}
The goal of the project will be to give a computational account of FILL. This 
includes considering the following problems: 
\begin{itemize}
\item 
A Curry-Howard interpretation of the pure logic as given by Bra\"{u}ner and 
V. de Paiva, \cite{BraunerdePaiva96}, should be developed. This includes 
obtaining an appropriate notion of reduction. The questions of operational 
semantics, evaluation strategies can then be addressed. Abramsky's proof-terms 
will not do as they do not deal with linear implication explicitly, but instead 
has negation built-in in a way such that it automatically is involutive.
\item 
Strong normalisation and Church-Rosser results for the Curry-Howard 
interpretation of FILL should be proved.
\item 
A non-trivial sound denotational semantics should be given in a way that explains 
the characteristic features of FILL.
\item 
The constructive nature of FILL should be clarified considering the questions of 
Church-Rosser and denotational semantics. This should be compared to the 
intuitionistic nature of Intuitionistic Linear Logic and contrasted to the 
non-intuitionistic, but constructive, nature of Classical Linear Logic. 
\item
The issue of recursion in the linear version of PCF dealt with in Bra\"{u}ner's 
Ph.D. dissertation should be considered in the more complicated setting of a term 
calculus for FILL.
\item 
A comparison with the classical system of M. Parigot, \cite{Parigot92}, should 
be attempted. In particular using the ideas of the $\lambda\mu$ calculus we ought 
to obtain classical linear logic from FILL. 
\item 
Several recent attempts at generalised notions for concurrency e.g. $\pi$-calculus 
and actions structures seem to use linear logic notions. It would be interesting 
to investigate whether FILL's notion of dependency between formulae (types) can be 
captured by action calculi and whether FILL can shed some light on the categorical 
modelling of such calculi. There is some work in progress in Edinburgh along these 
lines.
\end{itemize}


