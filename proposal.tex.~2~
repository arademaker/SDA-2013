
\section{The proposal}\label{sec:proposal}

As discussed in Section~\ref{sec:problems}, relational databases are
often hard to maintain, scale and share. Also, the idea of having
``in-house developed'' and closed source information systems is being
increasingly replaced by the concept of open source systems. In such
systems the responsibility of updating and creating new features is
not sustained by a single institution but usually by a whole community
that share knowledge and interests with associates. In this way the
system is keept up-to-date, accessible and growing much faster due to
the increased number of contributors. Such systems are usually
compatible with standards in a way to garantee their widely adoption.

In this context we suggest that CPDOC improves the way their rich
historical data is accessed, stored and shared. Our proposal
privileges open source systems and a lightweight, shared way of
dealing with data.

% It is important to mention that CPDOC has currently three different
% information systems. Althought in the first stage the pilot project
% will involve only the DHBB system, this article is about the whole
% vision of the CPDOC data migration. This system was chosen due to
% different administrative reasons, but also because it has the
% simplest data model.

Concretely, we propose the substitution of the three CPDOC systems by
the following technologies.

The Acessus and PHO systems data and files would be stored in an open
source institutional repository software such as
Dspace~\footnote{\url{http://www.dspace.org/}}, Fedora Commons
Framework~\footnote{\url{http://www.fedora-commons.org}} or a more
specialized repository management system like
ICA-AtoM~\cite{van2009ica}~\footnote{\url{https://www.ica-atom.org}}. In
this article we assume the adoption of Dspace with no prejudice of
theoretical modelling. The Acessus data model comprises personal
archives that contains one or more series (which can contain also
other series in a stratified hierarchy) of digitalized documents or
photographies. The PHO system data model is basically a set of
interviews grouped according to some defined criteria within the
context given by funded projects. For instance, a political event
could originate a project which involve interviewing many important
people taking part on the event.

% These projects are the motivation and origin of financial support
% for a series of interviews conducted by CPDOC team with a specific
% purpose.

It is possible to consider that Acessus and PHO systems are basically
responsible for maintaining collections of documents organized in a
hierarchical structure. In this way, one can assume that any digital
repository management system have all required
functionalities. Besides that, digital repository management systems
share features desired but not present in Acessus or PHO, such as: (1)
standard data model based on standard vocabularies like Dublin
Core~\cite{dc} and SKOS~\cite{skos}; (2) long-term data preservation
functionalities (tracking and notifications of chances in files); (3)
fine-grained access control policies; (4) flexible user interface for
basic and advanced queries; (5) compliance with standard protocols for
repositories synchronization and interoperability (e.g.,
OAI-PMH~\cite{oai} protocol); and more.

When it comes to the DHBB system, its relational model is a couple of
tables that store metadata about the dictionary entries (stored in a
single table). The actual dictionary entries are created and edited in
text editors outside the system and imported to the system after the
whole process of creation and revision. In this way, the system is not
able to maintain the history of changes. The entries are saved in
HTML~\cite{html} format in the database. The HTML is generated by the
different text editors used making it not uniform nor clean.

The nature of its data suggests that DHBB entries could be easily
maintained as text files using a lightweight human-readble markup
syntax. The files would be organized in a intuitive directory
structure and kept under version control for coordinated and
distributed maintenance.

Most of the suggested technologies are of daily use to most people
with technical profile, such as software developers, but not very
familiar to people with non-technical profile. In this context, a big
challenge of this approach is to motivate the internal users of CPDOC
systems, i.e., CPDOC archives maintainers, to invest their time to
learn new technologies, instead of keeping their inefficient but known
way of dealing with data.

This proposal ideas are already implemented partially for evaluation
purpose only. Figure~\ref{fig:dia-1} illustrates the necessary steps
to full implement our proposal. In the next pagraphs we will briefly
describe each step.

In step (1) the relational database was exported to RDF~\cite{rdf}
using the open source D2RQ~\cite{d2rq} tool. The D2RQ mapping
language~\cite{d2rq-map} allows the definition of a detailed mapping
from the current relational model to a graph model based on RDF. We
have already defined the mapping from the relational model to RDF
model which uses the standard translation from relational to RDF model
sketched out in \cite{dbANDrdf}. The mapping created so far defers any
model improvement to step (3) described below. Until CPDOC team decide
about the adoption of the proposed technologies and for the
abandonment of the current architecture, the generated RDF can be
updated whenever necessary using D2RQ.

In step (2) the digital files and their metadata will be imported into
the digital repository management system. This step will be much
easier to be done using the RDF produced in step (1) than having to
access the original database. For complete this step we only have to
decide about the repository management system that will be adopted,
install it in a server and create the necessary scripts. Considering
that all digital repository management systems have detailed control
access mecanisms, we believe that both high and low resolution files
can be imported to the same digital repository making only the low
resolution open for public access. This is necessary mainly for
preserve bandwidth and because in general, the low resolution files
also contain some watermark and embeded metadadata.

In step (3) we will create a refinement of the graph data model
produced in step (1). The ideia of the refinement is to produce a data
model based on standard vocabularies like Dublin Core~\cite{dc},
SKOS~\cite{skos}, PROV~\cite{prov} and FOAF~\cite{foaf}. The use of
standard vocabularies will make it interchangeable with other models
and facilitate its adoption by service provides and users. In
Section~\ref{sec:model} we describe in more detail the idea of this
refinement.

In step (4) we produced one text files for each DHBB entry. Each text
file hold the text of the entry and its metadata. The files use
YAML~\cite{yaml} and Markdown~\cite{markdown} markup languages for
describe the metadata and the actuall content of the entry. We choose
to adopt YAML and Markdown because both languages are human-readable
and have parsers to translatate them to a final presentation format
such as HMTL~\cite{html}. We have actually adopted a static web site
generator~\footnote{We used Jekyll, \url{http://jekyllrb.com}, but any
  other one could also be used.} for generate a complete web site for
DHBB from the text files.

% and implements most of the ideas currently recommended by the W3C's
% R2RML mapping language~\cite{r2rml}.

\begin{figure}[thbp]
  \centering
  \includegraphics[width=.9\textwidth]{diagrama1.png}
  \caption{Data migration from relational databases to the proposed model}\label{fig:dia-1}
\end{figure}

 The migration of the data from relational
databases to RDF~\cite{rdf} and a simple prototype for browse the data
was implemented using Jekyll
System~\footnote{\url{http://jekyllrb.com}} and query support was
provided by Apache Solr~\cite{solr} was built and used. The result
obtained encouraged us to proposed a more complete model, proposed in
this paper, which would be useful for CPDOC current needs.

% FICOU MUITO FORA DE CONTEXTO To make CPDOC data widely used and
% available, we also suggest that CPDOC should define a annual
% schedule for distribution snapshots of its archives in RDF
% format. The RDF file(s) could be offer for download and online
% available for queries in a triple store with a SPARQL endpoint. In
% the next section we further describe the proposal architecture.

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "article_revA"
%%% End: 
